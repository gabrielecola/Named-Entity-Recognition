{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b4f7ab",
   "metadata": {},
   "source": [
    "### Named Entity Recognition of a Corpus\n",
    "The purpose of this project is to analyze the Georgetown University Multilayer (**GUM**) Corpus  coming from this [repo](https://github.com/nluninja/nlp_datasets/tree/main/GUM), that contains two columns:\n",
    "\n",
    "- token\n",
    "- ner_tag\n",
    "\n",
    "This [corpus](https://github.com/amir-zeldes/gum) contains English texts from twelve written and spoken text types:\n",
    "- interviews\n",
    "- news\n",
    "- travel guides\n",
    "- how-to guides\n",
    "- academic writing\n",
    "- biographies\n",
    "- fiction\n",
    "- online forum discussions\n",
    "- spontaneous face to face conversations\n",
    "- political speeches\n",
    "- textbooks\n",
    "- vlogs\n",
    "\n",
    "Our goal is to classify correctly the 23 classes coming from the **ner_tag** through a **BILSTM** and **Bert Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444925f",
   "metadata": {},
   "source": [
    "### 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca77fdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielecola/anaconda3/envs/NER/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing\n",
    "import re\n",
    "import urllib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Preprocessing\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from math import nan\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import en_core_web_sm\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# EDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation,NMF\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# BILSTM\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizerFast,BertForTokenClassification\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "\n",
    "from Functions.importing import load_conll_data\n",
    "from Functions.preprocessing import flatten_list, display_topics, remove_seq_padding, from_encode_to_literal_labels\n",
    "from Functions.model import load_glove_embedding_matrix, bilstm\n",
    "from Functions.plot import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c7db4",
   "metadata": {},
   "source": [
    "### 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('/Users/gabrielecola/NER_2/data')\n",
    "raw_train, ner_train, output_labels = load_conll_data('gum-train.conll', dir_path=data_dir, only_tokens=True)\n",
    "raw_test, ner_test, _ = load_conll_data('gum-test.conll', dir_path=data_dir, only_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e30067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = pd.DataFrame({'Sentence': raw_train})\n",
    "sentence_data['Sentence'] = [' '.join(map(str, l)) for l in sentence_data['Sentence']]\n",
    "sentence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb31017",
   "metadata": {},
   "source": [
    "### 2. Pre- Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e22048",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = flatten_list(raw_train)\n",
    "ner_new = flatten_list(ner_train)\n",
    "ner_data = pd.DataFrame(\n",
    "    {'Word': train_new,\n",
    "     'Tag': ner_new\n",
    "    })\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb164a",
   "metadata": {},
   "source": [
    "### 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb868c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= str(train_new)\n",
    "testo=\" \".join(train_new)\n",
    "stopwords = ['many','one','will','may','see','make','well']+ list(STOPWORDS)\n",
    "testo2 = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=2000,\n",
    "    stopwords=stopwords,\n",
    "    width=1600, height=800\n",
    ")\n",
    "# generate the word cloud\n",
    "testo2.generate(testo)\n",
    "\n",
    "# display the word cloud\n",
    "plt.figure( figsize=(15,15))\n",
    "plt.title('WordCloud')\n",
    "plt.imshow(testo2, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de198cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "# Create the count plot ordered by the counts\n",
    "ax = sns.countplot(\n",
    "    y='Tag', \n",
    "    data=ner_data,\n",
    "    order=ner_data['Tag'].value_counts().index\n",
    ")\n",
    "ax.set_title('The count of each of the 23 classes')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc9c08",
   "metadata": {},
   "source": [
    "### 4. Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3890157",
   "metadata": {},
   "source": [
    "#### 4.1 Pre-process The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_topic = pd.DataFrame({'Sentence': raw_train})\n",
    "data_topic['Sentence'] = [' '.join(map(str, l)) for l in data_topic['Sentence']]\n",
    "data_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer is used to convert a collection of text documents to a matrix of token counts\n",
    "# The result is a sparse matrix where each row corresponds to a document and each column corresponds to a token\n",
    "# The value in each cell is the count of the token in the document\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "dtm = cv.fit_transform(data_topic['Sentence'])\n",
    "dtm_feature_names = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20904b",
   "metadata": {},
   "source": [
    "#### 4.2 LDA\n",
    "\n",
    "LDA used for **Topic Modeling**, it is a generative **probabilistic** model that assumes each **document** is a mixture of **topics** and each topic is a mixture of words. \\\n",
    "The model learns the topics from the data and can be used to infer the topic distribution of new documents. \\\n",
    "Furthermore The model is trained using the Expectation-Maximization algorithm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=12,random_state=42) # The number of topics is a hyperparameter that needs to be set before training the model\n",
    "LDA.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2b2d3",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58f48c9",
   "metadata": {},
   "source": [
    "#### 5.1 Token Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec74dee",
   "metadata": {},
   "source": [
    "An ordinal encoding involves mapping each unique label to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "token_tokenizer = Tokenizer()    # Automatically lowers tokens\n",
    "token_tokenizer.fit_on_texts(raw_train + raw_test)\n",
    "# Save the tokenizer\n",
    "with open('/Users/gabrielecola/NER_2/tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(token_tokenizer, file)\n",
    "\n",
    "train_sequences = token_tokenizer.texts_to_sequences(raw_train)\n",
    "test_sequences = token_tokenizer.texts_to_sequences(raw_test)\n",
    "\n",
    "tag2idx = { tag: idx for idx, tag in enumerate(output_labels) }\n",
    "idx2tag = { idx: tag for tag, idx in tag2idx.items() }\n",
    "ner_train_sequences = [[tag2idx[tag] for tag in sentence] for sentence in ner_train]\n",
    "ner_test_sequences  = [[tag2idx[tag] for tag in sentence] for sentence in ner_test ]\n",
    "\n",
    "vocabulary_size = len(token_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_test[5])\n",
    "print(test_sequences[5])\n",
    "for i in test_sequences[5]:\n",
    "    print(f'{i} : {token_tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63452484",
   "metadata": {},
   "source": [
    "#### 5.2 Sequence Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25502960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to specify the maximum length of each sequence you can use the maxlen argument. This will truncate all sequences longer than maxlen.\n",
    "max_sequence_len = 50\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_len, padding='post', truncating='post')\n",
    "\n",
    "Y_train = pad_sequences(ner_train_sequences, maxlen=max_sequence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
    "Y_test = pad_sequences(ner_test_sequences, maxlen=max_sequence_len, value=tag2idx['O'], padding='post', truncating='post')\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes=len(output_labels), dtype='int32')\n",
    "Y_test = to_categorical(Y_test, num_classes=len(output_labels), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb0539",
   "metadata": {},
   "source": [
    "#### 5.3 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9487e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e0611",
   "metadata": {},
   "source": [
    "### 6. Model: BILSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11bdf0",
   "metadata": {},
   "source": [
    "#### 6.1 Glove: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GLOVE=True\n",
    "glove_matrix=None\n",
    "if USE_GLOVE:\n",
    "    glove_embedding_path = '/Users/gabrielecola/NER_2/glove.6B/glove.6B.100d.txt'\n",
    "    embedding_dim = 100\n",
    "    glove_matrix = load_glove_embedding_matrix(glove_embedding_path, token_tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= bilstm(vocabulary_size+1, # vocabulary size + 1 for padding,\n",
    "              max_sequence_len, # maximum length of the sequences\n",
    "              drop=0.6, # dropout rate\n",
    "              hidden_cells=200, # number of hidden cells\n",
    "              n_classes= len(output_labels),\n",
    "              use_glove=USE_GLOVE,\n",
    "              glove_matrix=glove_matrix)\n",
    "\n",
    "\n",
    "es=EarlyStopping(monitor='loss',patience=2,mode=\"auto\")\n",
    "history = model.fit(X_train, Y_train, batch_size=10, epochs=100, verbose=2, callbacks=[es])\n",
    "model.save('/Users/gabrielecola/NER_2/bilstm.h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b98b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy\n",
    "loss = history.history['loss']\n",
    "accuracy = history.history['accuracy']\n",
    "\n",
    "# (Optional) validation data\n",
    "val_loss = history.history.get('val_loss')\n",
    "val_accuracy = history.history.get('val_accuracy')\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(loss, label='Training Loss', color='red', marker='o')\n",
    "if val_loss:\n",
    "    plt.plot(val_loss, label='Validation Loss', color='orange', marker='x')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(accuracy, label='Training Accuracy', color='blue', marker='o')\n",
    "if val_accuracy:\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy', color='green', marker='x')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Training Loss and Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8899669",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "datasets = [('Training Set', X_train, Y_train), ('Test Set', X_test, Y_test)]\n",
    "for title, X, Y in datasets:\n",
    "    # we make predictions\n",
    "    Y_pred_lstm = model.predict(X, batch_size=batch_size)\n",
    "    Y_pred_lstm = np.array(np.argmax(Y_pred_lstm, axis=-1))\n",
    "    Y = np.array(np.argmax(Y, axis=-1))\n",
    "    Y, Y_pred = remove_seq_padding(X, Y, Y_pred_lstm)\n",
    "    let_y_true_lstm, let_y_pred_lstm = from_encode_to_literal_labels(Y, Y_pred_lstm, idx2tag)\n",
    "    # from a double list we make a single list according to the argument of classification report\n",
    "    single_list_true_lstm = []\n",
    "    single_list_pred_lstm = []\n",
    "    for i in range(len(let_y_true_lstm)):\n",
    "      for j in range(len(let_y_true_lstm[i])):\n",
    "        single_list_true_lstm.append(let_y_true_lstm[i][j])\n",
    "    for i in range(len(let_y_pred_lstm)):\n",
    "      for j in range(len(let_y_pred_lstm[i])):\n",
    "       single_list_pred_lstm.append(let_y_pred_lstm[i][j])\n",
    "    print(title)\n",
    "    print(classification_report(single_list_pred_lstm, single_list_true_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes= ['B-abstract','B-animal','B-event','B-object','B-organization','B-person','B-place','B-plant','B-quantity',\n",
    "          'B-substance','B-time','I-abstract','I-animal','I-event','I-object','I-organization','I-person','I-place',\n",
    "          'I-plant','I-quantity','I-substance','I-time','O']\n",
    "\n",
    "\n",
    "# calculated on test set\n",
    "plot_confusion_matrix(single_list_pred_lstm, single_list_true_lstm, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d0003",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length',\n",
    "                                  max_length=self.max_len,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        labels_enc = np.ones(self.max_len, dtype=int) * -100\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        label_ids = [tag2idx[l] for l in label]\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_idx = 0\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                if word_idx != previous_word_idx:\n",
    "                    labels_enc[i] = label_ids[label_idx]\n",
    "                    label_idx += 1\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(labels_enc, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataset = NERDataset(raw_train, ner_train, tokenizer, MAX_LEN)\n",
    "test_dataset = NERDataset(raw_test, ner_test, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# =====================\n",
    "# 3. Model: BERT for NER\n",
    "# =====================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(output_labels))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# =====================\n",
    "# 4. Training (1 Epoch Example)\n",
    "# =====================\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "# =====================\n",
    "# 5. Evaluation\n",
    "# =====================\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            true = labels[i].cpu().numpy()\n",
    "            pred = preds[i].cpu().numpy()\n",
    "\n",
    "            mask = true != -100\n",
    "            true_labels.extend(true[mask])\n",
    "            pred_labels.extend(pred[mask])\n",
    "\n",
    "# =====================\n",
    "# 6. Confusion Matrix & Report\n",
    "# =====================\n",
    "print(\"Classification Report - BERT\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=output_labels))\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=output_labels)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')\n",
    "plt.title('Confusion Matrix - BERT Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
